# BMTrain--背景介绍

如何高效、并行、简单训练模型

CPU：包含少量的大计算单元

GPU：包含大量的小计算单元

大模型中显存如何消耗：

<ul>
<li>参数量（1）</li>
<li>梯度（1）</li>
<li>中间计算结果</li>
<li>优化器（>2）：如adam</li>
</ul>

比如11B的模型：参数本身约占40G显存，若要全参数调整则至少需要5倍的显存
11\*10^9\*4(fp32)/2^30 =40G 

# 数据并行

多张显卡划分数据进行训练，但有可能参数量和梯度以及优化器显存量过大，一张显卡无法使用（仅仅改变batch的大小，即影响中间过程参数减少）

# 模型并行  

矩阵分解：分解模型参数，大大减少了参数量和梯度、优化器大小，中间结果无法变小

 
